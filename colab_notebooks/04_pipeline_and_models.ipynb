{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_pipeline_and_models_V3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAg7rpWSIH2y",
        "colab_type": "text"
      },
      "source": [
        "# Pipeline and models\n",
        "The final steps. Prepare the features to be consumed by different models that we'll train and test, and finally present the results\n",
        "\n",
        "#### Steps:\n",
        "- [x] Load the basic features from step 03\n",
        "- [x] Split the dataset into train and test \n",
        "- [x] Normalize the data to be trained\n",
        "- [x] Train and test different models and architectures\n",
        "- [x] Find the best results and summarize them\n",
        "\n",
        "#### The pipeline\n",
        "Polinomial features > Normalization > Train (multiple models)\n",
        "\n",
        "#### Prerequisites for this notebook:\n",
        "- Go through `03_feature_engineering.ipynb` so the raw parsed datasets are available "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0PdFiA9H64T",
        "colab_type": "code",
        "outputId": "7640d705-5e11-4bee-efa2-44f74d26ac27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# First we must mount google drive \n",
        "from google.colab import drive\n",
        "\n",
        "GDRIVE_BASE_PATH = '/content/gdrive'\n",
        "drive.mount(GDRIVE_BASE_PATH)\n",
        "\n",
        "# Loading our project setup\n",
        "HOME_DIR = '/content/gdrive/My Drive/project_scs3253'\n",
        "% cd $HOME_DIR\n",
        "\n",
        "from util.project_setup import ProjectSetup\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "RANDOM_SEED = 1\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "SCORING_METRICS = 'f1'\n",
        "PICK_TOP_N_MODELS = 5\n",
        "CV_COUNT = 3 \n",
        "POLYNOMIAL_DEGREE = 2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/My Drive/project_scs3253\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uqe2WN5lMsnB",
        "colab_type": "code",
        "outputId": "7e31cefb-77b9-4b0b-9570-cb5093b01167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# Loading the dataframe with basic features\n",
        "\n",
        "#df = pd.read_csv(f'{ProjectSetup.data_dir}/basic_features_dataframe_file.csv', sep=',', index_col=0)\n",
        "\n",
        "# We are using the files that is already generated by the previos step. It is renamed here to remind us what\n",
        "# filtering we did. (1000ms holdtime cutoff, 1000 min observation, mild impact only)\n",
        "df = pd.read_csv(f'{ProjectSetup.data_dir}/filter_1000holdtime_1000obs_mildonly.csv', sep=',', index_col=0)\n",
        "df = df.drop(\"userkey\", axis=1)\n",
        "\n",
        "# Show the breakdown to see if we get a balanced dataset\n",
        "print(\"Parkinsons:\\n\", df['parkinsons'].value_counts())\n",
        "\n",
        "y = df.parkinsons\n",
        "X = df.drop('parkinsons', axis=1)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parkinsons:\n",
            " True     43\n",
            "False    41\n",
            "Name: parkinsons, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNeQp_qQKR21",
        "colab_type": "code",
        "outputId": "ddb620ce-d542-413d-844d-ede051158fd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Spliting with 20% on the test set - as we don't have lots of data, if we keep more for test, we can't train\n",
        "# good models\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y)\n",
        "\n",
        "print(\"Parkinsons:\\n\", y_train.value_counts())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parkinsons:\n",
            " True     34\n",
            "False    33\n",
            "Name: parkinsons, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXAXnaO_k2El",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a wrapper class responsible to delegate the fit/predict \n",
        "# calls to the underlying estimator. This decorator helps us to \n",
        "# train different ML models on a single pipeline and select the best\n",
        "from sklearn.base import BaseEstimator\n",
        "class EstimatorDecorator(BaseEstimator):\n",
        "  def __init__(self, estimator=None):\n",
        "    self.estimator = estimator\n",
        "\n",
        "  def fit(self, X, y=None, **kwargs):\n",
        "    self.estimator.fit(X, y)\n",
        "    return self\n",
        "\n",
        "  def predict(self, X, y=None):\n",
        "    return self.estimator.predict(X)\n",
        "\n",
        "  def predict_proba(self, X):\n",
        "    return self.estimator.predict_proba(X)\n",
        "\n",
        "  def score(self, X, y):\n",
        "    return self.estimator.score(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU-5OG3JmDna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now let's build our pipeline using our decorator classifier\n",
        "#from imblearn.pipeline import Pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model.ridge import RidgeClassifier\n",
        "from sklearn.linear_model.passive_aggressive import PassiveAggressiveClassifier\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neighbors.classification import RadiusNeighborsClassifier\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "training_pipeline = Pipeline(verbose=False, steps=[\n",
        "  ('pol', PolynomialFeatures(degree = POLYNOMIAL_DEGREE)),\n",
        "  ('nor', Normalizer()),\n",
        "  ('clf', EstimatorDecorator())\n",
        "])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sqAMBQMOqLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's create some smart functions to train all models and report the results\n",
        "# (we might want to move this later to our `util` module)\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pprint as pp\n",
        "import collections\n",
        "import re\n",
        "\n",
        "MyEmptydf = pd.DataFrame()\n",
        "\n",
        "def class_to_string(klass):\n",
        "  return re.match(r'\\A(?P<class_name>\\w+)\\(', str(klass)).group('class_name')\n",
        "\n",
        "\n",
        "def calculate_scores(predictions, y):\n",
        "  \n",
        "  confusion = confusion_matrix(y, predictions)    \n",
        "  tn, fp, fn, tp = confusion.ravel()\n",
        "  specificity = tn / (tn + fp)\n",
        "  \n",
        "  scores = {\n",
        "    'f1'         : f1_score(y, predictions),\n",
        "    'precision'  : precision_score(y, predictions),\n",
        "    'recall'     : recall_score(y, predictions),\n",
        "    'specificity': round(specificity, 6),\n",
        "    'accuracy'   : accuracy_score(y, predictions),\n",
        "    'auc'        : roc_auc_score(y, predictions),\n",
        "    'confusion'  : confusion,\n",
        "  }\n",
        "  return scores\n",
        "\n",
        "\n",
        "def print_scores(scores_tr, scores_te):\n",
        "  if (scores_te['accuracy']==0) and (scores_te['f1']==0):\n",
        "    print(f\"> Scores     <train>    | <NoTest>\")\n",
        "  else:\n",
        "    print(f\"> Scores     <train>    | <test>\")\n",
        "  print(f\"> F1         : {scores_tr['f1']:f} | {scores_te['f1']:f}\")\n",
        "  print(f\"> Precision  : {scores_tr['precision']:f} | {scores_te['precision']:f}\")\n",
        "  print(f\"> Recall     : {scores_tr['recall']:f} | {scores_te['recall']:f}\")\n",
        "  print(f\"> Specificity: {scores_tr['specificity']:f} | {scores_te['specificity']:f}\")\n",
        "  print(f\"> Accuracy   : {scores_tr['accuracy']:f} | {scores_te['accuracy']:f}\")\n",
        "  print(f\"> AUC        : {scores_tr['auc']:f} | {scores_te['auc']:f}\")\n",
        "  print(f\"> ** Confusion matrix (Train)**\")\n",
        "  print(scores_tr['confusion'])\n",
        "  if (scores_te['f1'] > 0 and scores_te['accuracy']) > 0 :\n",
        "    print(f\"> ** Confusion matrix (Test)**\")\n",
        "    print(scores_te['confusion'])\n",
        "\n",
        "    \n",
        "def calculate_train_and_test_scores(predictor, X_train, y_train, X_test=MyEmptydf, y_test=MyEmptydf):\n",
        "  predictions = predictor.predict(X_train)\n",
        "  train_scores = calculate_scores(predictions, y_train)\n",
        "\n",
        "  # Only show y_test score if we passed in y_test\n",
        "  if y_test.empty:\n",
        "    test_scores = calculate_scores(~y_train, y_train)\n",
        "    f1_delta = 0.0 # not considering\n",
        "  else:  \n",
        "    predictions = predictor.predict(X_test)\n",
        "    test_scores = calculate_scores(predictions, y_test)\n",
        "    f1_delta = train_scores['f1'] - test_scores['f1']\n",
        "  \n",
        "  return {\n",
        "    'f1_delta': f1_delta,\n",
        "    'train_scores': train_scores,\n",
        "    'test_scores': test_scores,\n",
        "  }\n",
        "\n",
        "\n",
        "def extract_grid_params(cv_results_params_dict):\n",
        "  old_params = collections.OrderedDict(sorted(cv_results_params_dict.items()))\n",
        "  \n",
        "  extrated_params = {}\n",
        "  for param, value in old_params.items():\n",
        "    if param == 'clf__estimator':\n",
        "      extrated_params['>estimator<'] = class_to_string(value[0])\n",
        "      continue\n",
        "      \n",
        "    param = param.replace('clf__estimator__', '')\n",
        "    extrated_params[param] = value[0]\n",
        "  \n",
        "  return extrated_params\n",
        "\n",
        "\n",
        "def train_models(X, y, pipeline, param_grid, cv, scoring):\n",
        "  grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring=scoring, return_train_score=True, iid=False, n_jobs=-1)\n",
        "  grid_search.fit(X, y)\n",
        "  return grid_search\n",
        "  \n",
        "\n",
        "def score_best_models(cv_results, X_train, y_train, X_test, y_test, pipeline, param_grid, cv, scoring,\n",
        "                     # mean_test_score_threshold=0.84, f1_delta_threshold=0.025):\n",
        "                       mean_test_score_threshold=0.5, f1_delta_threshold=1): # we cannot use f1_delta to filter models as it is using the test data\n",
        "  scores = {}\n",
        "  index = -1\n",
        "  for rank in cv_results['rank_test_score']:\n",
        "    index += 1\n",
        "    if cv_results['mean_test_score'][index] >= mean_test_score_threshold:\n",
        "      param_grid = cv_results['params'][index].copy()\n",
        "      for key, value in param_grid.items(): param_grid[key] = [value] \n",
        "      grid_search = train_models(X_train, y_train, pipeline, param_grid, cv, scoring)\n",
        "      \n",
        "      grid_pipeline = grid_search.best_estimator_\n",
        "      estimator_name = class_to_string(grid_pipeline['clf'].estimator)\n",
        "      estimator_scores = calculate_train_and_test_scores(grid_pipeline, X_train, y_train, X_test=X_test, y_test=y_test)\n",
        "      \n",
        "      if (abs(estimator_scores['f1_delta'])) <= f1_delta_threshold:\n",
        "        scores[f'{index} - {estimator_name}'] = {\n",
        "          'estimator': grid_pipeline,\n",
        "          'params'   : extract_grid_params(param_grid),\n",
        "          'scores'   : estimator_scores,\n",
        "        }\n",
        "  return scores\n",
        "\n",
        "\n",
        "def print_always_true_scores(y_train, y_test):\n",
        "  print('\\n\\n>>>>>> Comparing with our base estimator (always predicts true):')\n",
        "  scores_tr = calculate_scores(np.ones(y_train.shape), y_train )\n",
        "  scores_te = calculate_scores(np.ones(y_test.shape), y_test )\n",
        "  print_scores(scores_tr, scores_te)\n",
        "\n",
        "  \n",
        "def train_models_and_select_best(X_train, y_train, X_test, y_test, pipeline, param_grid, cv=CV_COUNT, scoring=SCORING_METRICS, print_params=True):\n",
        "  grid_search = train_models(X_train, y_train, pipeline, param_grid, cv, scoring)\n",
        "  \n",
        "  print('\\n>>>>>> How each estimator is scoring:')\n",
        "  print(f\"Rank/position:   {grid_search.cv_results_['rank_test_score']}\")\n",
        "  print(f\"Mean test score: {grid_search.cv_results_['mean_test_score']}\")\n",
        "  \n",
        "  print('\\n\\n>>>>>> Auto-selecting the best performers')\n",
        "  best_models = score_best_models(grid_search.cv_results_, X_train, y_train, X_test, y_test, pipeline, param_grid, cv, scoring)\n",
        "  \n",
        "  print(' Sensitivity/recall: how good the model is at detecting the positives')\n",
        "  print(' Specificity       : how good the model is at avoiding false alarms')\n",
        "  print(' Precision         : how many of the positively classified were relevant')\n",
        "  \n",
        "  for model_name, params in best_models.items():\n",
        "    print(f\"\\nModel: {model_name}\")\n",
        "    if print_params: pp.pprint(params['params'])\n",
        "    score = params['scores']\n",
        "    print(f\"> F1 delta (train-test): {score['f1_delta']:f}\")\n",
        "    print_scores(score['train_scores'], score['test_scores'])\n",
        "  \n",
        "  print_always_true_scores(y_train, y_test)\n",
        "\n",
        "  return best_models, grid_search\n",
        "\n",
        "\n",
        "def pick_top_n_models(cv_results, max_models, X_train, y_train, pipeline, param_grid, cv, scoring):\n",
        "  scores = {}\n",
        "  index = -1\n",
        "  \n",
        "  print(\"We also reject the models who never predicts any positive or negative case correctly.\")\n",
        "  for rank in cv_results['rank_test_score']:\n",
        "    index += 1\n",
        "    # only show models what is on the top max_models\n",
        "    if rank <= max_models:\n",
        "\n",
        "      param_grid = cv_results['params'][index].copy()\n",
        "      for key, value in param_grid.items(): param_grid[key] = [value]\n",
        "\n",
        "      grid_search_temp = train_models(X_train, y_train, pipeline, param_grid, cv, scoring)\n",
        "\n",
        "      grid_pipeline = grid_search_temp.best_estimator_\n",
        "      estimator_name = class_to_string(grid_pipeline['clf'].estimator) + ' (Rank ' + str(rank) + ')'\n",
        "      estimator_scores = calculate_train_and_test_scores(grid_pipeline, X_train, y_train)\n",
        "      # only want those with specificity > 0, otherwise we will pick models that never predict correctly any false\n",
        "      if ((estimator_scores['train_scores']['specificity'] > 0) and (estimator_scores['train_scores']['precision'] > 0)):   \n",
        "        scores[f'{index} - {estimator_name}'] = {\n",
        "          'estimator': grid_pipeline,\n",
        "          'params'   : extract_grid_params(param_grid),\n",
        "          'scores'   : estimator_scores,\n",
        "        }\n",
        "      else:\n",
        "        print (\"Reject model {0} {1} (precision:{2}, specificity:{3})\".format(index, \n",
        "                                                                 estimator_name,                         \n",
        "                                                                 estimator_scores['train_scores']['precision'],\n",
        "                                                                 estimator_scores['train_scores']['specificity']))\n",
        "  return scores\n",
        "\n",
        "\n",
        "def train_and_select_any_top_n(X_train, y_train, pipeline, param_grid, max_models=PICK_TOP_N_MODELS, cv=CV_COUNT, scoring=SCORING_METRICS, print_params=True):\n",
        "  grid_search = train_models(X_train, y_train, pipeline, param_grid, cv, scoring)\n",
        " \n",
        "  print('\\n>>>>>> How each estimator is scoring:')\n",
        "  print(f\"Rank/position:   {grid_search.cv_results_['rank_test_score']}\")\n",
        "  print(f\"Mean test score: {grid_search.cv_results_['mean_test_score']}\")\n",
        "    \n",
        "  print(' Sensitivity/recall: how good the model is at detecting the positives')\n",
        "  print(' Specificity       : how good the model is at avoiding false alarms')\n",
        "  print(' Precision         : how many of the positively classified were relevant')\n",
        "  \n",
        "  print('\\n\\n>>>>>>showing top {} models'.format(max_models) )\n",
        "  best_models = pick_top_n_models(grid_search.cv_results_, max_models, X_train, y_train, pipeline, param_grid, cv, scoring)\n",
        "  for model_name, params in best_models.items():\n",
        "    print(f\"\\nModel: {model_name}\")\n",
        "    if print_params: pp.pprint(params['params'])\n",
        "    score = params['scores']\n",
        "    print(f\"> F1 delta (train-test): {score['f1_delta']:f}\")\n",
        "    print_scores(score['train_scores'], score['test_scores'])\n",
        "  \n",
        "  print('\\n\\n>>>>>> Comparing with our base estimator (always predicts true):')\n",
        "  scores_tr = calculate_scores(np.ones(y_train.shape), y_train )\n",
        "  scores_te = calculate_scores(np.ones(y_test.shape), y_test )\n",
        "  print_scores(scores_tr, scores_te)\n",
        "\n",
        "  return best_models, grid_search\n",
        "\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "def creating_VotingClassifier(clf_list, option):\n",
        "  temp_list = []\n",
        "  for best_model_name, best_model_dict in clf_list.items():\n",
        "    temp_list.append((best_model_name, best_model_dict['estimator']['clf'].estimator))\n",
        "  return (VotingClassifier(estimators=temp_list, voting=option))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsSAaVkDDlBj",
        "colab_type": "code",
        "outputId": "26130f46-1d32-4ea3-e61f-ff238d414fcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "param_grid_all_features = [\n",
        "  # SVC has the tuned parameters\n",
        "  {\n",
        "    'clf__estimator': [SVC()],\n",
        "    'clf__estimator__random_state': [RANDOM_SEED],\n",
        "    'clf__estimator__probability': [True],\n",
        "    'clf__estimator__kernel': [\"poly\"],\n",
        "    'clf__estimator__degree' : [3], \n",
        "    'clf__estimator__gamma': ['scale'],  \n",
        "    'clf__estimator__C': [0.1],\n",
        "    'clf__estimator__decision_function_shape': ['ovo'],      \n",
        "  },\n",
        "  # RandomForestClassifier has the tuned parameters\n",
        "  {\n",
        "    'clf__estimator': [RandomForestClassifier()],\n",
        "    'clf__estimator__random_state':      [RANDOM_SEED],\n",
        "    'clf__estimator__bootstrap':         [True],\n",
        "    'clf__estimator__class_weight':      [None],\n",
        "    'clf__estimator__criterion':         ['entropy'],\n",
        "    'clf__estimator__max_depth':         [2],\n",
        "    'clf__estimator__max_features':      ['auto'], \n",
        "    'clf__estimator__n_estimators':      [20],\n",
        "  },\n",
        "  # LogisticRegression has tuned parameters  \n",
        "  {\n",
        "    'clf__estimator': [LogisticRegression()],\n",
        "    'clf__estimator__random_state': [RANDOM_SEED],\n",
        "    'clf__estimator__C': [0.9],\n",
        "    'clf__estimator__multi_class': ['ovr'],\n",
        "    'clf__estimator__solver': ['lbfgs'],\n",
        "    'clf__estimator__max_iter': [1000],      \n",
        "  },      \n",
        "  #  DecisionTreeClassifier has tuned parameters \n",
        "  {\n",
        "    'clf__estimator': [DecisionTreeClassifier()],\n",
        "    'clf__estimator__random_state': [RANDOM_SEED],\n",
        "    'clf__estimator__criterion':    ['entropy'],\n",
        "    'clf__estimator__max_depth':    [2],\n",
        "  },   \n",
        " \n",
        "#   {\n",
        "#     'clf__estimator': [GradientBoostingClassifier()],\n",
        "#     'clf__estimator__random_state': [RANDOM_SEED],\n",
        "#   },\n",
        "  #  GradientBoostingClassifier has tuned parameters   \n",
        "  {\n",
        "    'clf__estimator': [GradientBoostingClassifier()],\n",
        "    'clf__estimator__random_state': [RANDOM_SEED],\n",
        "    'clf__estimator__loss': ['exponential'],\n",
        "    'clf__estimator__learning_rate':     [5],\n",
        "    'clf__estimator__n_estimators' : [100],\n",
        "    'clf__estimator__max_depth' : [5],\n",
        "    'clf__estimator__max_features' :['auto']\n",
        "  },   \n",
        "#   {    \n",
        "#     'clf__estimator': [XGBClassifier()],\n",
        "#     'clf__estimator__random_state':      [RANDOM_SEED],\n",
        "#   },\n",
        "    \n",
        "  # XGBClassifier with tuned parameters\n",
        "  {\n",
        "    'clf__estimator': [XGBClassifier()],\n",
        "    'clf__estimator__random_state':      [RANDOM_SEED],\n",
        "    'clf__estimator__max_depth':         [3],\n",
        "    'clf__estimator__learning_rate':     [10],\n",
        "    'clf__estimator__n_estimators':      [20], \n",
        "    'clf__estimator__booster':           ['gbtree'], \n",
        "  },\n",
        "\n",
        "  # KNeighborsClassifier has tuned parameters  \n",
        "  {\n",
        "    'clf__estimator': [KNeighborsClassifier()],\n",
        "    'clf__estimator__n_neighbors': [4],\n",
        "  },\n",
        "  #   AdaBoostClassifier has the tuned parameters\n",
        "  {\n",
        "    'clf__estimator': [AdaBoostClassifier()],\n",
        "    'clf__estimator__random_state': [RANDOM_SEED],\n",
        "    'clf__estimator__n_estimators':      [100],\n",
        "    'clf__estimator__learning_rate':     [0.1],\n",
        "    'clf__estimator__algorithm': ['SAMME.R'],\n",
        "  },       \n",
        "  {\n",
        "    'clf__estimator': [GaussianNB()],\n",
        "  },   \n",
        "]\n",
        "\n",
        "# We use the mean_score of cross_validation to select models because we have very few samples in the dataset and if we split further\n",
        "# for test, model_sel, test, we won't be able to train the model properly.\n",
        "# best_models1, grid_search1 = train_models_and_select_best(X_train, y_train, X_test, y_test, training_pipeline, param_grid_all_features, cv=CV_COUNT, scoring=SCORING_METRICS)\n",
        "\n",
        "# find the best n models \n",
        "best_models1, grid_search1 = train_and_select_any_top_n(X_train, y_train,training_pipeline, param_grid_all_features, cv=CV_COUNT, scoring=SCORING_METRICS)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">>>>>> How each estimator is scoring:\n",
            "Rank/position:   [9 3 8 4 6 2 7 5 1]\n",
            "Mean test score: [0.17777778 0.46805007 0.22539683 0.39692982 0.32206119 0.61206349\n",
            " 0.27961973 0.33108171 0.65151515]\n",
            " Sensitivity/recall: how good the model is at detecting the positives\n",
            " Specificity       : how good the model is at avoiding false alarms\n",
            " Precision         : how many of the positively classified were relevant\n",
            "\n",
            "\n",
            ">>>>>>showing top 5 models\n",
            "We also reject the models who never predicts any positive or negative case correctly.\n",
            "Reject model 5 XGBClassifier (Rank 2) (precision:0.5074626865671642, specificity:0.0)\n",
            "\n",
            "Model: 1 - RandomForestClassifier (Rank 3)\n",
            "{'>estimator<': 'RandomForestClassifier',\n",
            " 'bootstrap': True,\n",
            " 'class_weight': None,\n",
            " 'criterion': 'entropy',\n",
            " 'max_depth': 2,\n",
            " 'max_features': 'auto',\n",
            " 'n_estimators': 20,\n",
            " 'random_state': 1}\n",
            "> F1 delta (train-test): 0.000000\n",
            "> Scores     <train>    | <NoTest>\n",
            "> F1         : 0.888889 | 0.000000\n",
            "> Precision  : 0.965517 | 0.000000\n",
            "> Recall     : 0.823529 | 0.000000\n",
            "> Specificity: 0.969697 | 0.000000\n",
            "> Accuracy   : 0.895522 | 0.000000\n",
            "> AUC        : 0.896613 | 0.000000\n",
            "> ** Confusion matrix (Train)**\n",
            "[[32  1]\n",
            " [ 6 28]]\n",
            "\n",
            "Model: 3 - DecisionTreeClassifier (Rank 4)\n",
            "{'>estimator<': 'DecisionTreeClassifier',\n",
            " 'criterion': 'entropy',\n",
            " 'max_depth': 2,\n",
            " 'random_state': 1}\n",
            "> F1 delta (train-test): 0.000000\n",
            "> Scores     <train>    | <NoTest>\n",
            "> F1         : 0.790698 | 0.000000\n",
            "> Precision  : 0.653846 | 0.000000\n",
            "> Recall     : 1.000000 | 0.000000\n",
            "> Specificity: 0.454545 | 0.000000\n",
            "> Accuracy   : 0.731343 | 0.000000\n",
            "> AUC        : 0.727273 | 0.000000\n",
            "> ** Confusion matrix (Train)**\n",
            "[[15 18]\n",
            " [ 0 34]]\n",
            "\n",
            "Model: 7 - AdaBoostClassifier (Rank 5)\n",
            "{'>estimator<': 'AdaBoostClassifier',\n",
            " 'algorithm': 'SAMME.R',\n",
            " 'learning_rate': 0.1,\n",
            " 'n_estimators': 100,\n",
            " 'random_state': 1}\n",
            "> F1 delta (train-test): 0.000000\n",
            "> Scores     <train>    | <NoTest>\n",
            "> F1         : 1.000000 | 0.000000\n",
            "> Precision  : 1.000000 | 0.000000\n",
            "> Recall     : 1.000000 | 0.000000\n",
            "> Specificity: 1.000000 | 0.000000\n",
            "> Accuracy   : 1.000000 | 0.000000\n",
            "> AUC        : 1.000000 | 0.000000\n",
            "> ** Confusion matrix (Train)**\n",
            "[[33  0]\n",
            " [ 0 34]]\n",
            "\n",
            "Model: 8 - GaussianNB (Rank 1)\n",
            "{'>estimator<': 'GaussianNB'}\n",
            "> F1 delta (train-test): 0.000000\n",
            "> Scores     <train>    | <NoTest>\n",
            "> F1         : 0.704545 | 0.000000\n",
            "> Precision  : 0.574074 | 0.000000\n",
            "> Recall     : 0.911765 | 0.000000\n",
            "> Specificity: 0.303030 | 0.000000\n",
            "> Accuracy   : 0.611940 | 0.000000\n",
            "> AUC        : 0.607398 | 0.000000\n",
            "> ** Confusion matrix (Train)**\n",
            "[[10 23]\n",
            " [ 3 31]]\n",
            "\n",
            "\n",
            ">>>>>> Comparing with our base estimator (always predicts true):\n",
            "> Scores     <train>    | <test>\n",
            "> F1         : 0.673267 | 0.692308\n",
            "> Precision  : 0.507463 | 0.529412\n",
            "> Recall     : 1.000000 | 1.000000\n",
            "> Specificity: 0.000000 | 0.000000\n",
            "> Accuracy   : 0.507463 | 0.529412\n",
            "> AUC        : 0.500000 | 0.500000\n",
            "> ** Confusion matrix (Train)**\n",
            "[[ 0 33]\n",
            " [ 0 34]]\n",
            "> ** Confusion matrix (Test)**\n",
            "[[0 8]\n",
            " [0 9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3qbcpu7EOmr",
        "colab_type": "text"
      },
      "source": [
        "### Now create a VotingClassifier with the top n models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDVdY0HY3emA",
        "colab_type": "code",
        "outputId": "7727b72a-0f9f-45ff-84f7-d9a3b40e88e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "# Create a pipeline for using the voting classifiers\n",
        "allfeatures_pipeline = Pipeline(verbose=False, steps=[\n",
        "       ('pol', PolynomialFeatures(degree = POLYNOMIAL_DEGREE)),\n",
        "       ('nor', Normalizer()),\n",
        "       ('clf', creating_VotingClassifier(best_models1, \"soft\") )\n",
        "      ])\n",
        "\n",
        "#print_always_true_scores(y_train, y_test)\n",
        "#print(\"============== \")\n",
        "allfeatures_pipeline.fit(X_train, y_train)\n",
        "print(\"All features score:\", allfeatures_pipeline.score(X_test,y_test)) \n",
        "y_tr_pred = allfeatures_pipeline.predict(X_train)\n",
        "y_te_pred = allfeatures_pipeline.predict(X_test)\n",
        "scores_tr = calculate_scores(y_tr_pred, y_train)\n",
        "scores_te = calculate_scores(y_te_pred, y_test)\n",
        "print_scores(scores_tr, scores_te)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All features score: 0.8235294117647058\n",
            "> Scores     <train>    | <test>\n",
            "> F1         : 0.850000 | 0.857143\n",
            "> Precision  : 0.739130 | 0.750000\n",
            "> Recall     : 1.000000 | 1.000000\n",
            "> Specificity: 0.636364 | 0.625000\n",
            "> Accuracy   : 0.820896 | 0.823529\n",
            "> AUC        : 0.818182 | 0.812500\n",
            "> ** Confusion matrix (Train)**\n",
            "[[21 12]\n",
            " [ 0 34]]\n",
            "> ** Confusion matrix (Test)**\n",
            "[[5 3]\n",
            " [0 9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3md9o5n62eT6",
        "colab_type": "text"
      },
      "source": [
        "Our best results (that is not overfit for training data):\n",
        "\n",
        "Top models: \n",
        "- GaussianNB (Rank 1)\n",
        "- (Rejected) XGBClassifier (Rank 2) \n",
        "- RandomForestClassifier (Rank 3)\n",
        "- DecisionTreeClassifier (Rank 4)\n",
        "- AdaBoostClassifier (Rank 5)\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "* SCORING_METRICS = 'f1'\n",
        "* PICK_TOP_N_MODELS = 5\n",
        "* CV_COUNT = 3 \n",
        "* POLYNOMIAL_DEGREE = 2\n",
        "\n",
        "All features score: 0.8235294117647058\n",
        "> Scores     <train>    | <test>\n",
        "> F1         : 0.850000 | 0.857143\n",
        "> Precision  : 0.739130 | 0.750000\n",
        "> Recall     : 1.000000 | 1.000000\n",
        "> Specificity: 0.636364 | 0.625000\n",
        "> Accuracy   : 0.820896 | 0.823529\n",
        "> AUC        : 0.818182 | 0.812500\n",
        "  \n",
        " ** Confusion matrix (Train)**\n",
        "[[20 13]\n",
        " [ 0 34]]\n",
        " ** Confusion matrix (Test)**\n",
        "[[4 4]\n",
        " [0 9]]\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfJLH--75eIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
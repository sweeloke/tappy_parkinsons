{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"04_pipeline_and_models_V3.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"oAg7rpWSIH2y","colab_type":"text"},"source":["# Pipeline and models\n","The final steps. Prepare the features to be consumed by different models that we'll train and test, and finally present the results\n","\n","#### Steps:\n","- [x] Load the basic features from step 03\n","- [x] Split the dataset into train and test \n","- [x] Normalize the data to be trained\n","- [x] Train and test different models and architectures\n","- [x] Find the best results and summarize them\n","\n","#### The pipeline\n","Polinomial features > Normalization > Train (multiple models)\n","\n","#### Prerequisites for this notebook:\n","- Go through `03_feature_engineering.ipynb` so the raw parsed datasets are available "]},{"cell_type":"code","metadata":{"id":"q0PdFiA9H64T","colab_type":"code","outputId":"698a8967-3dae-46bd-dd44-3e773937a12a","executionInfo":{"status":"ok","timestamp":1566047374737,"user_tz":240,"elapsed":1657,"user":{"displayName":"Swee Loke","photoUrl":"https://lh5.googleusercontent.com/-wRQ0l56UUWo/AAAAAAAAAAI/AAAAAAAAAOA/yXvSf6W6ros/s64/photo.jpg","userId":"17662899385833089467"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# First we must mount google drive \n","from google.colab import drive\n","\n","GDRIVE_BASE_PATH = '/content/gdrive'\n","drive.mount(GDRIVE_BASE_PATH)\n","\n","# Loading our project setup\n","HOME_DIR = '/content/gdrive/My Drive/project_scs3253'\n","% cd $HOME_DIR\n","\n","from util.project_setup import ProjectSetup\n","import numpy as np\n","import pandas as pd\n","\n","RANDOM_SEED = 1\n","np.random.seed(RANDOM_SEED)\n","\n","SCORING_METRICS = 'f1'\n","#SCORING_METRICS = 'accuracy'\n","\n","PICK_TOP_N_MODELS = 3\n","CV_COUNT = 3 \n","POLYNOMIAL_DEGREE = 2"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/My Drive/project_scs3253\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Uqe2WN5lMsnB","colab_type":"code","outputId":"710eb0c5-7e10-44c5-8511-3035f2b09803","executionInfo":{"status":"ok","timestamp":1566047375021,"user_tz":240,"elapsed":1919,"user":{"displayName":"Swee Loke","photoUrl":"https://lh5.googleusercontent.com/-wRQ0l56UUWo/AAAAAAAAAAI/AAAAAAAAAOA/yXvSf6W6ros/s64/photo.jpg","userId":"17662899385833089467"}},"colab":{"base_uri":"https://localhost:8080/","height":86}},"source":["# Loading the dataframe with basic features\n","#df = pd.read_csv(f'{ProjectSetup.data_dir}/basic_features_dataframe_file.csv', sep=',', index_col=0)\n","\n","df = pd.read_csv(f'{ProjectSetup.data_dir}/filter_1000holdtime_1000obs_mildonly.csv', sep=',', index_col=0)\n","df = df.drop(\"userkey\", axis=1)\n","\n","# Show the breakdown to see if we get a balanced dataset\n","print(\"Parkinsons:\\n\", df['parkinsons'].value_counts())\n","\n","y = df.parkinsons\n","X = df.drop('parkinsons', axis=1)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Parkinsons:\n"," True     43\n","False    41\n","Name: parkinsons, dtype: int64\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nNeQp_qQKR21","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":86},"outputId":"5f6d39e0-6c57-4d4d-daa0-fe01bb12e89c","executionInfo":{"status":"ok","timestamp":1566047375761,"user_tz":240,"elapsed":2628,"user":{"displayName":"Swee Loke","photoUrl":"https://lh5.googleusercontent.com/-wRQ0l56UUWo/AAAAAAAAAAI/AAAAAAAAAOA/yXvSf6W6ros/s64/photo.jpg","userId":"17662899385833089467"}}},"source":["# Spliting with 25% on the test set\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y)\n","#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=24, stratify=y)\n","\n","print(\"Parkinsons:\\n\", y_train.value_counts())"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Parkinsons:\n"," True     34\n","False    33\n","Name: parkinsons, dtype: int64\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RXAXnaO_k2El","colab_type":"code","colab":{}},"source":["# Creating a wrapper class responsible to delegate the fit/predict \n","# calls to the underlying estimator. This decorator helps us to \n","# train different ML models on a single pipeline and select the best\n","from sklearn.base import BaseEstimator\n","class EstimatorDecorator(BaseEstimator):\n","  def __init__(self, estimator=None):\n","    self.estimator = estimator\n","\n","  def fit(self, X, y=None, **kwargs):\n","    self.estimator.fit(X, y)\n","    return self\n","\n","  def predict(self, X, y=None):\n","    return self.estimator.predict(X)\n","\n","  def predict_proba(self, X):\n","    return self.estimator.predict_proba(X)\n","\n","  def score(self, X, y):\n","    return self.estimator.score(X, y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EU-5OG3JmDna","colab_type":"code","colab":{}},"source":["# Now let's build our pipeline using our decorator classifier\n","#from imblearn.pipeline import Pipeline\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.preprocessing import Normalizer\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.linear_model.ridge import RidgeClassifier\n","from sklearn.linear_model.passive_aggressive import PassiveAggressiveClassifier\n","from sklearn.linear_model import Perceptron\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import ExtraTreesClassifier\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import BaggingClassifier\n","\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.neighbors.classification import RadiusNeighborsClassifier\n","from sklearn.neighbors import NearestCentroid\n","\n","from sklearn.svm import SVC\n","from sklearn.svm import LinearSVC\n","\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.naive_bayes import BernoulliNB\n","\n","from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","\n","from sklearn.gaussian_process import GaussianProcessClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","\n","training_pipeline = Pipeline(verbose=False, steps=[\n","  ('pol', PolynomialFeatures(degree = POLYNOMIAL_DEGREE)),\n","  ('nor', Normalizer()),\n","  ('clf', EstimatorDecorator())\n","])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7sqAMBQMOqLK","colab_type":"code","colab":{}},"source":["# Let's create some smart functions to train all models and report the results\n","# (we might want to move this later to our `util` module)\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import GridSearchCV\n","import pprint as pp\n","import collections\n","import re\n","\n","MyEmptydf = pd.DataFrame()\n","\n","def class_to_string(klass):\n","  return re.match(r'\\A(?P<class_name>\\w+)\\(', str(klass)).group('class_name')\n","\n","\n","def calculate_scores(predictions, y):\n","  \n","  confusion = confusion_matrix(y, predictions)    \n","  tn, fp, fn, tp = confusion.ravel()\n","  specificity = tn / (tn + fp)\n","  \n","  scores = {\n","    'f1'         : f1_score(y, predictions),\n","    'precision'  : precision_score(y, predictions),\n","    'recall'     : recall_score(y, predictions),\n","    'specificity': round(specificity, 6),\n","    'accuracy'   : accuracy_score(y, predictions),\n","    'auc'        : roc_auc_score(y, predictions),\n","    'confusion'  : confusion,\n","  }\n","  return scores\n","\n","def print_scores(scores_tr, scores_te):\n","  if (scores_te['accuracy']==0) and (scores_te['f1']==0):\n","    print(f\"> Scores     <train>    | <NoTest>\")\n","  else:\n","    print(f\"> Scores     <train>    | <test>\")\n","  print(f\"> F1         : {scores_tr['f1']:f} | {scores_te['f1']:f}\")\n","  print(f\"> Precision  : {scores_tr['precision']:f} | {scores_te['precision']:f}\")\n","  print(f\"> Recall     : {scores_tr['recall']:f} | {scores_te['recall']:f}\")\n","  print(f\"> Specificity: {scores_tr['specificity']:f} | {scores_te['specificity']:f}\")\n","  print(f\"> Accuracy   : {scores_tr['accuracy']:f} | {scores_te['accuracy']:f}\")\n","  print(f\"> AUC        : {scores_tr['auc']:f} | {scores_te['auc']:f}\")\n","  print(f\"> ** Confusion matrix (Train)**\")\n","  print(scores_tr['confusion'])\n","  if (scores_te['f1'] > 0 and scores_te['accuracy']) > 0 :\n","    print(f\"> ** Confusion matrix (Test)**\")\n","    print(scores_te['confusion'])\n","\n","def calculate_train_and_test_scores(predictor, X_train, y_train, X_test=MyEmptydf, y_test=MyEmptydf):\n","  predictions = predictor.predict(X_train)\n","  train_scores = calculate_scores(predictions, y_train)\n","\n","  # Only show y_test score if we passed in y_test\n","  if y_test.empty:\n","    test_scores = calculate_scores(~y_train, y_train)\n","    f1_delta = 0.0 # not considering\n","  else:  \n","    predictions = predictor.predict(X_test)\n","    test_scores = calculate_scores(predictions, y_test)\n","    f1_delta = train_scores['f1'] - test_scores['f1']\n","  \n","  return {\n","    'f1_delta': f1_delta,\n","    'train_scores': train_scores,\n","    'test_scores': test_scores,\n","  }\n","\n","def extract_grid_params(cv_results_params_dict):\n","  old_params = collections.OrderedDict(sorted(cv_results_params_dict.items()))\n","  \n","  extrated_params = {}\n","  for param, value in old_params.items():\n","    if param == 'clf__estimator':\n","      extrated_params['>estimator<'] = class_to_string(value[0])\n","      continue\n","      \n","    param = param.replace('clf__estimator__', '')\n","    extrated_params[param] = value[0]\n","  \n","  return extrated_params\n","\n","\n","def train_models(X, y, pipeline, param_grid, cv, scoring):\n","  grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring=scoring, return_train_score=True, iid=False, n_jobs=-1)\n","  grid_search.fit(X, y)\n","  return grid_search\n","  \n","\n","def score_best_models(cv_results, X_train, y_train, X_test, y_test, pipeline, param_grid, cv, scoring,\n","                     # mean_test_score_threshold=0.84, f1_delta_threshold=0.025):\n","                       mean_test_score_threshold=0.5, f1_delta_threshold=1): # we cannot use f1_delta to filter models as it is using the test data\n","  scores = {}\n","  index = -1\n","  for rank in cv_results['rank_test_score']:\n","    index += 1\n","    if cv_results['mean_test_score'][index] >= mean_test_score_threshold:\n","      param_grid = cv_results['params'][index].copy()\n","      for key, value in param_grid.items(): param_grid[key] = [value] \n","      grid_search = train_models(X_train, y_train, pipeline, param_grid, cv, scoring)\n","      \n","      grid_pipeline = grid_search.best_estimator_\n","      estimator_name = class_to_string(grid_pipeline['clf'].estimator)\n","      estimator_scores = calculate_train_and_test_scores(grid_pipeline, X_train, y_train, X_test=X_test, y_test=y_test)\n","      \n","      if (abs(estimator_scores['f1_delta'])) <= f1_delta_threshold:\n","        scores[f'{index} - {estimator_name}'] = {\n","          'estimator': grid_pipeline,\n","          'params'   : extract_grid_params(param_grid),\n","          'scores'   : estimator_scores,\n","        }\n","  return scores\n","\n","def print_always_true_scores(y_train, y_test):\n","  print('\\n\\n>>>>>> Comparing with our base estimator (always predicts true):')\n","  scores_tr = calculate_scores(np.ones(y_train.shape), y_train )\n","  scores_te = calculate_scores(np.ones(y_test.shape), y_test )\n","  print_scores(scores_tr, scores_te)\n","  \n","def train_models_and_select_best(X_train, y_train, X_test, y_test, pipeline, param_grid, cv=CV_COUNT, scoring=SCORING_METRICS, print_params=True):\n","  grid_search = train_models(X_train, y_train, pipeline, param_grid, cv, scoring)\n","  \n","  print('\\n>>>>>> How each estimator is scoring:')\n","  print(f\"Rank/position:   {grid_search.cv_results_['rank_test_score']}\")\n","  print(f\"Mean test score: {grid_search.cv_results_['mean_test_score']}\")\n","  \n","  print('\\n\\n>>>>>> Auto-selecting the best performers')\n","  best_models = score_best_models(grid_search.cv_results_, X_train, y_train, X_test, y_test, pipeline, param_grid, cv, scoring)\n","  \n","  print(' Sensitivity/recall: how good the model is at detecting the positives')\n","  print(' Specificity       : how good the model is at avoiding false alarms')\n","  print(' Precision         : how many of the positively classified were relevant')\n","  \n","  for model_name, params in best_models.items():\n","    print(f\"\\nModel: {model_name}\")\n","    if print_params: pp.pprint(params['params'])\n","    score = params['scores']\n","    print(f\"> F1 delta (train-test): {score['f1_delta']:f}\")\n","    print_scores(score['train_scores'], score['test_scores'])\n","  \n","  print_always_true_scores(y_train, y_test)\n","\n","  return best_models, grid_search\n","\n","\n","def pick_top_n_models(grid_search, max_models, X_train, y_train, pipeline, param_grid, cv, scoring):\n","  scores = {}\n","  index = -1\n","  cv_results = grid_search.cv_results_\n","  \n","  for rank in cv_results['rank_test_score']:\n","    index += 1\n","    # only show models what is on the top max_models\n","    if rank <= max_models:\n","      param_grid = cv_results['params'][index].copy()\n","      for key, value in param_grid.items(): param_grid[key] = [value]\n","\n","      grid_search_temp = train_models(X_train, y_train, pipeline, param_grid, cv, scoring)\n","\n","      grid_pipeline = grid_search_temp.best_estimator_\n","      estimator_name = class_to_string(grid_pipeline['clf'].estimator)\n","      estimator_scores = calculate_train_and_test_scores(grid_pipeline, X_train, y_train)\n","\n","      scores[f'{index} - {estimator_name}'] = {\n","        'estimator': grid_pipeline,\n","        'params'   : extract_grid_params(param_grid),\n","        'scores'   : estimator_scores,\n","      }\n","  return scores\n","\n","def train_and_select_any_top_n(X_train, y_train, pipeline, param_grid, max_models=PICK_TOP_N_MODELS, cv=CV_COUNT, scoring=SCORING_METRICS, print_params=True):\n","  grid_search = train_models(X_train, y_train, pipeline, param_grid, cv, scoring)\n"," \n","  print('\\n>>>>>> How each estimator is scoring:')\n","  print(f\"Rank/position:   {grid_search.cv_results_['rank_test_score']}\")\n","  print(f\"Mean test score: {grid_search.cv_results_['mean_test_score']}\")\n","    \n","  print(' Sensitivity/recall: how good the model is at detecting the positives')\n","  print(' Specificity       : how good the model is at avoiding false alarms')\n","  print(' Precision         : how many of the positively classified were relevant')\n","  \n","  print('\\n\\n>>>>>>showing top {} models'.format(max_models) )\n","  best_models = pick_top_n_models(grid_search, max_models, X_train, y_train, pipeline, param_grid, cv, scoring)\n","  for model_name, params in best_models.items():\n","    print(f\"\\nModel: {model_name}\")\n","    if print_params: pp.pprint(params['params'])\n","    score = params['scores']\n","    print(f\"> F1 delta (train-test): {score['f1_delta']:f}\")\n","    print_scores(score['train_scores'], score['test_scores'])\n","  \n","  print('\\n\\n>>>>>> Comparing with our base estimator (always predicts true):')\n","  scores_tr = calculate_scores(np.ones(y_train.shape), y_train )\n","  scores_te = calculate_scores(np.ones(y_test.shape), y_test )\n","  print_scores(scores_tr, scores_te)\n","\n","  return best_models, grid_search"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wsSAaVkDDlBj","colab_type":"code","outputId":"2e33484e-9f95-45ec-d1d7-b0af37be1aa5","executionInfo":{"status":"ok","timestamp":1566047386990,"user_tz":240,"elapsed":13724,"user":{"displayName":"Swee Loke","photoUrl":"https://lh5.googleusercontent.com/-wRQ0l56UUWo/AAAAAAAAAAI/AAAAAAAAAOA/yXvSf6W6ros/s64/photo.jpg","userId":"17662899385833089467"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["param_grid_all_features = [\n","        \n","#     # This is the parameters we found with individual search before, we need to find the same using this algorithm\n","#     clf = SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n","#     decision_function_shape='ovo', degree=3, gamma='scale', kernel='poly',\n","#     max_iter=-1, probability=True, random_state=RANDOM_SEED, shrinking=True, tol=0.001,\n","#     verbose=False)\n","  {\n","    'clf__estimator': [SVC()],\n","    'clf__estimator__random_state': [RANDOM_SEED],\n","    'clf__estimator__probability': [True],\n","    'clf__estimator__kernel': [\"poly\"],\n","    'clf__estimator__degree' : [3], \n","    'clf__estimator__gamma': ['scale'],  \n","    'clf__estimator__C': [0.1],\n","    'clf__estimator__decision_function_shape': ['ovo'],      \n","  },\n","# # Comment out the following temporary for testing SVC search    \n","  {\n","    'clf__estimator': [RandomForestClassifier()],\n","    'clf__estimator__random_state': [RANDOM_SEED],\n","    'clf__estimator__n_estimators': [100],\n","    'clf__estimator__max_depth':    [2],\n","  },\n","  {\n","    'clf__estimator': [RandomForestClassifier()],\n","    'clf__estimator__random_state':      [RANDOM_SEED],\n","    'clf__estimator__bootstrap':         [True],\n","    'clf__estimator__class_weight':      [None],\n","    #'clf__estimator__criterion':         ['gini','entropy'],\n","    'clf__estimator__criterion':         ['entropy'],\n","    'clf__estimator__max_depth':         [2, 5, 7],\n","    #'clf__estimator__max_depth':         [2,3],\n","    'clf__estimator__max_features':      ['auto'], \n","    'clf__estimator__n_estimators':      [20, 50, 100],\n","    'clf__estimator__oob_score':         [True,False],\n","  },\n","  {\n","    'clf__estimator': [LogisticRegression()],\n","    'clf__estimator__random_state': [RANDOM_SEED],\n","    'clf__estimator__C': [0.1, 0.5, 1],\n","    'clf__estimator__solver': ['lbfgs', 'liblinear'],\n","    'clf__estimator__max_iter': [1000],\n","  },      \n","    \n","  {\n","    'clf__estimator': [DecisionTreeClassifier()],\n","    'clf__estimator__random_state': [RANDOM_SEED],\n","    'clf__estimator__criterion':    ['gini','entropy'],\n","    #'clf__estimator__criterion':   ['entropy'],\n","    'clf__estimator__max_depth':    [2, 3],\n","  },    \n","  {\n","    'clf__estimator': [GradientBoostingClassifier()],\n","    'clf__estimator__random_state': [RANDOM_SEED],\n","  },\n","  {\n","    'clf__estimator': [KNeighborsClassifier()],\n","    'clf__estimator__n_neighbors': [5],\n","  },\n","  {\n","    'clf__estimator': [AdaBoostClassifier()],\n","    'clf__estimator__random_state': [RANDOM_SEED],\n","  },\n","        \n","  {\n","    'clf__estimator': [GaussianNB()],\n","  },\n","#####################################    \n","# We don't use the following   \n","#       {\n","#     'clf__estimator': [SVC()],\n","#     'clf__estimator__random_state': [RANDOM_SEED],\n","#     'clf__estimator__probability': [True], \n","#     'clf__estimator__kernel': ['poly', 'rbf'],\n","#     'clf__estimator__degree' : [2, 3], \n","#     'clf__estimator__gamma': ['scale'],  \n","#     'clf__estimator__C': [0.05, 0.1, 0.5],\n","#     'clf__estimator__decision_function_shape': ['ovo', 'ovr'],      \n","#   },    \n","#   {\n","#     'clf__estimator': [SGDClassifier()],\n","#     'clf__estimator__random_state': [RANDOM_SEED],\n","#     'clf__estimator__loss': ['hinge', 'log'],\n","#     'clf__estimator__alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n","#     'clf__estimator__epsilon': [0.0001, 0.001, 0.01, 0.1, 1],\n","#     'clf__estimator__learning_rate': ['constant','optimal', 'invscaling', 'adaptive' ], \n","#     'clf__estimator__eta0': [0.001], \n","#   },  \n","#   {\n","#     'clf__estimator': [MLPClassifier()],\n","#     'clf__estimator__random_state': [RANDOM_SEED],\n","#   },\n","#   {\n","#     'clf__estimator': [GaussianProcessClassifier()],\n","#     'clf__estimator__random_state': [RANDOM_SEED],\n","#   },\n","#   {\n","#     'clf__estimator': [QuadraticDiscriminantAnalysis()],\n","#   },\n","#   {\n","#     'clf__estimator': [BernoulliNB()],\n","#   },\n","#   {\n","#     'clf__estimator': [LinearDiscriminantAnalysis()],\n","#   },\n","#   {\n","#     'clf__estimator': [BaggingClassifier()],\n","#     'clf__estimator__random_state': [RANDOM_SEED],\n","#   },\n","#   {\n","#     'clf__estimator': [ExtraTreesClassifier()],\n","#     'clf__estimator__random_state': [RANDOM_SEED],\n","#   },\n","#   {\n","#     'clf__estimator': [NearestCentroid()],\n","#   },\n","#   {\n","#     'clf__estimator': [PassiveAggressiveClassifier()],\n","#     'clf__estimator__random_state': [RANDOM_SEED],\n","#   },\n","#   {\n","#     'clf__estimator': [Perceptron()],\n","#     'clf__estimator__random_state': [RANDOM_SEED],\n","#   },\n","###############################    \n","]\n","# best_models1, grid_search1 = train_models_and_select_best(X_train, y_train, X_test, y_test, training_pipeline, param_grid_all_features, cv=CV_COUNT, scoring=SCORING_METRICS)\n","# find the best n models \n","best_models1, grid_search1 = train_and_select_any_top_n(X_train, y_train,training_pipeline, param_grid_all_features, cv=CV_COUNT, scoring=SCORING_METRICS)\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["\n",">>>>>> How each estimator is scoring:\n","Rank/position:   [30  5  6  6 16 16  3  3 25 25  8  8 18 18 22 22 13 13 20 20 30 30 30 30\n"," 29 28 12 10 24 11 27  2 15  1]\n","Mean test score: [0.17777778 0.48181818 0.46805007 0.46805007 0.43507473 0.43507473\n"," 0.50324943 0.50324943 0.39444444 0.39444444 0.46068376 0.46068376\n"," 0.42765568 0.42765568 0.40048309 0.40048309 0.44222222 0.44222222\n"," 0.40384615 0.40384615 0.17777778 0.17777778 0.17777778 0.17777778\n"," 0.22222222 0.26666667 0.44736842 0.45502646 0.39692982 0.45112782\n"," 0.351341   0.5040404  0.44074074 0.65151515]\n"," Sensitivity/recall: how good the model is at detecting the positives\n"," Specificity       : how good the model is at avoiding false alarms\n"," Precision         : how many of the positively classified were relevant\n","\n","\n",">>>>>>showing top 3 models\n","\n","Model: 6 - RandomForestClassifier\n","{'>estimator<': 'RandomForestClassifier',\n"," 'bootstrap': True,\n"," 'class_weight': None,\n"," 'criterion': 'entropy',\n"," 'max_depth': 2,\n"," 'max_features': 'auto',\n"," 'n_estimators': 100,\n"," 'oob_score': True,\n"," 'random_state': 1}\n","> F1 delta (train-test): 0.000000\n","> Scores     <train>    | <NoTest>\n","> F1         : 0.895522 | 0.000000\n","> Precision  : 0.909091 | 0.000000\n","> Recall     : 0.882353 | 0.000000\n","> Specificity: 0.909091 | 0.000000\n","> Accuracy   : 0.895522 | 0.000000\n","> AUC        : 0.895722 | 0.000000\n","> ** Confusion matrix (Train)**\n","[[30  3]\n"," [ 4 30]]\n","\n","Model: 7 - RandomForestClassifier\n","{'>estimator<': 'RandomForestClassifier',\n"," 'bootstrap': True,\n"," 'class_weight': None,\n"," 'criterion': 'entropy',\n"," 'max_depth': 2,\n"," 'max_features': 'auto',\n"," 'n_estimators': 100,\n"," 'oob_score': False,\n"," 'random_state': 1}\n","> F1 delta (train-test): 0.000000\n","> Scores     <train>    | <NoTest>\n","> F1         : 0.895522 | 0.000000\n","> Precision  : 0.909091 | 0.000000\n","> Recall     : 0.882353 | 0.000000\n","> Specificity: 0.909091 | 0.000000\n","> Accuracy   : 0.895522 | 0.000000\n","> AUC        : 0.895722 | 0.000000\n","> ** Confusion matrix (Train)**\n","[[30  3]\n"," [ 4 30]]\n","\n","Model: 31 - KNeighborsClassifier\n","{'>estimator<': 'KNeighborsClassifier', 'n_neighbors': 5}\n","> F1 delta (train-test): 0.000000\n","> Scores     <train>    | <NoTest>\n","> F1         : 0.756757 | 0.000000\n","> Precision  : 0.700000 | 0.000000\n","> Recall     : 0.823529 | 0.000000\n","> Specificity: 0.636364 | 0.000000\n","> Accuracy   : 0.731343 | 0.000000\n","> AUC        : 0.729947 | 0.000000\n","> ** Confusion matrix (Train)**\n","[[21 12]\n"," [ 6 28]]\n","\n","Model: 33 - GaussianNB\n","{'>estimator<': 'GaussianNB'}\n","> F1 delta (train-test): 0.000000\n","> Scores     <train>    | <NoTest>\n","> F1         : 0.704545 | 0.000000\n","> Precision  : 0.574074 | 0.000000\n","> Recall     : 0.911765 | 0.000000\n","> Specificity: 0.303030 | 0.000000\n","> Accuracy   : 0.611940 | 0.000000\n","> AUC        : 0.607398 | 0.000000\n","> ** Confusion matrix (Train)**\n","[[10 23]\n"," [ 3 31]]\n","\n","\n",">>>>>> Comparing with our base estimator (always predicts true):\n","> Scores     <train>    | <test>\n","> F1         : 0.673267 | 0.692308\n","> Precision  : 0.507463 | 0.529412\n","> Recall     : 1.000000 | 1.000000\n","> Specificity: 0.000000 | 0.000000\n","> Accuracy   : 0.507463 | 0.529412\n","> AUC        : 0.500000 | 0.500000\n","> ** Confusion matrix (Train)**\n","[[ 0 33]\n"," [ 0 34]]\n","> ** Confusion matrix (Test)**\n","[[0 8]\n"," [0 9]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2xn_fAbRozJc","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title\n","# default_clf_list = [('knn', KNeighborsClassifier()), \n","#                     ('rf', RandomForestClassifier(random_state=RANDOM_SEED)), \n","#                     ('dt', DecisionTreeClassifier(random_state=RANDOM_SEED)), \n","#                     ('lr', LogisticRegression(random_state=RANDOM_SEED)), \n","#                     ('svc', SVC(random_state=RANDOM_SEED)),\n","#                     ('ada', AdaBoostClassifier(random_state=RANDOM_SEED)),\n","#                     ('svc2', SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n","#     decision_function_shape='ovo', degree=3, gamma='scale', kernel='poly',\n","#     max_iter=-1, probability=True, random_state=RANDOM_SEED, shrinking=True, tol=0.001,\n","#     verbose=False))]\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xqqYqZ16HaWq","colab_type":"code","colab":{},"cellView":"form"},"source":["#@title\n","\n","# all_features_classifiers_f1  =  [  \n","#                                    ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=2, metric='minkowski',\n","#                                                                  metric_params=None, n_jobs=None, n_neighbors=2, p=3,\n","#                                                                  weights='distance')), \n","#                                    ('rf', RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n","#                                                                  max_depth=5, max_features='log2', max_leaf_nodes=None,\n","#                                                                  min_impurity_decrease=0.0, min_impurity_split=None,\n","#                                                                  min_samples_leaf=7, min_samples_split=2,\n","#                                                                  min_weight_fraction_leaf=0.0, n_estimators=100,\n","#                                                                  n_jobs=None, oob_score=False, random_state=RANDOM_SEED, verbose=0,\n","#                                                                  warm_start=False)), \n","#                                    ('dt', DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=2,\n","#                                                                  max_features=None, max_leaf_nodes=None,\n","#                                                                  min_impurity_decrease=0.0, min_impurity_split=None,\n","#                                                                  min_samples_leaf=1, min_samples_split=2,\n","#                                                                  min_weight_fraction_leaf=0.0, presort=False,\n","#                                                                  random_state=RANDOM_SEED, splitter='best')), \n","#                                    ('lr', LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n","#                                                                  intercept_scaling=1, l1_ratio=None, max_iter=1000,\n","#                                                                  multi_class='ovr', n_jobs=None, penalty='l2',\n","#                                                                  random_state=RANDOM_SEED, solver='liblinear', tol=0.0001, verbose=0,\n","#                                                                  warm_start=False)), \n","#                                    ('svc', SVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0,\n","#                                                                   decision_function_shape='ovo', degree=2, gamma='auto', kernel='poly',\n","#                                                                   max_iter=-1, probability=True, random_state=RANDOM_SEED, shrinking=True, tol=0.001,\n","#                                                                   verbose=False)),\n","#                                    ('ada', AdaBoostClassifier(algorithm='SAMME', base_estimator=None, learning_rate=1,\n","#                                                                   n_estimators=150, random_state=RANDOM_SEED))]\n","\n","\n","# all_features_classifiers_accuracy  =  [  \n","#                                    ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=2, metric='minkowski',\n","#                                                                  metric_params=None, n_jobs=None, n_neighbors=2, p=1,\n","#                                                                  weights='distance')), \n","#                                    ('rf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n","#                                                                  max_depth=5, max_features='log2', max_leaf_nodes=None,\n","#                                                                  min_impurity_decrease=0.0, min_impurity_split=None,\n","#                                                                  min_samples_leaf=5, min_samples_split=2,\n","#                                                                  min_weight_fraction_leaf=0.0, n_estimators=50,\n","#                                                                  n_jobs=None, oob_score=False, random_state=RANDOM_SEED, verbose=0,\n","#                                                                  warm_start=False)), \n","#                                    ('dt', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n","#                                                                  max_features=None, max_leaf_nodes=None,\n","#                                                                  min_impurity_decrease=0.0, min_impurity_split=None,\n","#                                                                  min_samples_leaf=1, min_samples_split=2,\n","#                                                                  min_weight_fraction_leaf=0.0, presort=False,\n","#                                                                  random_state=RANDOM_SEED, splitter='best')), \n","#                                    ('lr', LogisticRegression(C=0.9, class_weight=None, dual=False, fit_intercept=True,\n","#                                                                  intercept_scaling=1, l1_ratio=None, max_iter=1000,\n","#                                                                  multi_class='ovr', n_jobs=None, penalty='l2',\n","#                                                                  random_state=RANDOM_SEED, solver='lbfgs', tol=0.0001, verbose=0,\n","#                                                                  warm_start=False)), \n","#                                    ('svc', SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n","#                                                                  decision_function_shape='ovo', degree=3, gamma='scale', kernel='poly',\n","#                                                                  max_iter=-1, probability=True, random_state=RANDOM_SEED, shrinking=True, tol=0.001,\n","#                                                                  verbose=False)),\n","#                                    ('ada', AdaBoostClassifier(algorithm='SAMME', base_estimator=None, learning_rate=1,\n","#                                                                  n_estimators=100, random_state=RANDOM_SEED))]\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W3qbcpu7EOmr","colab_type":"text"},"source":["### Now create a VotingClassifier with the top n models"]},{"cell_type":"code","metadata":{"id":"trGaZOmy-4Gs","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import VotingClassifier\n","def creating_VotingClassifier(clf_list, option):\n","  temp_list = []\n","  for best_model_name, best_model_dict in clf_list.items():\n","    temp_list.append((best_model_name, best_model_dict['estimator']['clf'].estimator))\n","  return (VotingClassifier(estimators=temp_list, voting=option))\n","  \n","  \n","all_features_clf_v = creating_VotingClassifier(best_models1, \"soft\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MDVdY0HY3emA","colab_type":"code","outputId":"b18a3754-0229-4224-fea4-8a1e06dda440","executionInfo":{"status":"ok","timestamp":1566047387363,"user_tz":240,"elapsed":13896,"user":{"displayName":"Swee Loke","photoUrl":"https://lh5.googleusercontent.com/-wRQ0l56UUWo/AAAAAAAAAAI/AAAAAAAAAOA/yXvSf6W6ros/s64/photo.jpg","userId":"17662899385833089467"}},"colab":{"base_uri":"https://localhost:8080/","height":534}},"source":["##### all features pipeline\n","allfeatures_pipeline = Pipeline(verbose=False, steps=[\n","       ('pol', PolynomialFeatures(degree = POLYNOMIAL_DEGREE)),\n","       ('nor', Normalizer()),\n","       ('clf', all_features_clf_v )\n","      ])\n","\n","# sanity check\n","allfeatures_pipeline.fit(X_train, y_train)\n","print(\"All features:\", allfeatures_pipeline.score(X_test,y_test)) \n","y_tr_pred = allfeatures_pipeline.predict(X_train)\n","y_te_pred = allfeatures_pipeline.predict(X_test)\n","scores_tr = calculate_scores(y_tr_pred, y_train)\n","scores_te = calculate_scores(y_te_pred, y_test)\n","print_scores(scores_tr, scores_te)\n","\n","print_always_true_scores(y_train, y_test)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["All features: 0.7058823529411765\n","> Scores     <train>    | <test>\n","> F1         : 0.785714 | 0.782609\n","> Precision  : 0.660000 | 0.642857\n","> Recall     : 0.970588 | 1.000000\n","> Specificity: 0.484848 | 0.375000\n","> Accuracy   : 0.731343 | 0.705882\n","> AUC        : 0.727718 | 0.687500\n","> ** Confusion matrix (Train)**\n","[[16 17]\n"," [ 1 33]]\n","> ** Confusion matrix (Test)**\n","[[3 5]\n"," [0 9]]\n","\n","\n",">>>>>> Comparing with our base estimator (always predicts true):\n","> Scores     <train>    | <test>\n","> F1         : 0.673267 | 0.692308\n","> Precision  : 0.507463 | 0.529412\n","> Recall     : 1.000000 | 1.000000\n","> Specificity: 0.000000 | 0.000000\n","> Accuracy   : 0.507463 | 0.529412\n","> AUC        : 0.500000 | 0.500000\n","> ** Confusion matrix (Train)**\n","[[ 0 33]\n"," [ 0 34]]\n","> ** Confusion matrix (Test)**\n","[[0 8]\n"," [0 9]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_C5L8nUKD9Du","colab_type":"code","colab":{}},"source":["# 0.2 train_test_split\n","# SCORING_METRICS = 'f1'\n","# PICK_TOP_N_MODELS = 3\n","# CV_COUNT = 3 \n","# POLYNOMIAL_DEGREE = 3\n","# All features: 0.6470588235294118\n","# > Scores     <train>    | <test>\n","# > F1         : 0.727273 | 0.727273\n","# > Precision  : 0.592593 | 0.615385\n","# > Recall     : 0.941176 | 0.888889\n","# > Specificity: 0.333333 | 0.375000\n","# > Accuracy   : 0.641791 | 0.647059\n","# > AUC        : 0.637255 | 0.631944\n","# > ** Confusion matrix (Train)**\n","# [[11 22]\n","#  [ 2 32]]\n","# > ** Confusion matrix (Test)**\n","# [[3 5]\n","#  [1 8]]\n","\n","# 0.2 train_test_split\n","# SCORING_METRICS = 'f1'\n","# PICK_TOP_N_MODELS = 3\n","# CV_COUNT = 3 \n","# POLYNOMIAL_DEGREE = 2\n","# All features: 0.7058823529411765\n","# > Scores     <train>    | <test>\n","# > F1         : 0.785714 | 0.782609\n","# > Precision  : 0.660000 | 0.642857\n","# > Recall     : 0.970588 | 1.000000\n","# > Specificity: 0.484848 | 0.375000\n","# > Accuracy   : 0.731343 | 0.705882\n","# > AUC        : 0.727718 | 0.687500\n","# > ** Confusion matrix (Train)**\n","# [[16 17]\n","#  [ 1 33]]\n","# > ** Confusion matrix (Test)**\n","# [[3 5]\n","#  [0 9]]\n","\n","# 0.2 train_test_split\n","# SCORING_METRICS = 'accuracy'\n","# PICK_TOP_N_MODELS = 3\n","# CV_COUNT = 3 \n","# POLYNOMIAL_DEGREE = 2\n","#\n","# All features: 0.7058823529411765\n","# > Scores     <train>    | <test>\n","# > F1         : 0.765432 | 0.782609\n","# > Precision  : 0.659574 | 0.642857\n","# > Recall     : 0.911765 | 1.000000\n","# > Specificity: 0.515152 | 0.375000\n","# > Accuracy   : 0.716418 | 0.705882\n","# > AUC        : 0.713458 | 0.687500\n","# > ** Confusion matrix (Train)**\n","# [[17 16]\n","#  [ 3 31]]\n","# > ** Confusion matrix (Test)**\n","# [[3 5]\n","#  [0 9]]\n","\n","# >>>>>> Comparing with our base estimator (always predicts true):\n","# > Scores     <train>    | <test>\n","# > F1         : 0.673267 | 0.692308\n","# > Precision  : 0.507463 | 0.529412\n","# > Recall     : 1.000000 | 1.000000\n","# > Specificity: 0.000000 | 0.000000\n","# > Accuracy   : 0.507463 | 0.529412\n","# > AUC        : 0.500000 | 0.500000\n","# > ** Confusion matrix (Train)**\n","# [[ 0 33]\n","#  [ 0 34]]\n","# > ** Confusion matrix (Test)**\n","# [[0 8]\n","#  [0 9]]\n","\n","\n","\n","# 0.25 train_test_split\n","# SCORING_METRICS = 'accuracy'\n","# PICK_TOP_N_MODELS = 5\n","# CV_COUNT = 3 \n","# POLYNOMIAL_DEGREE = 2\n","\n","# All features: 0.7142857142857143\n","# > Scores     <train>    | <test>\n","# > F1         : 0.941176 | 0.769231\n","# > Precision  : 0.888889 | 0.666667\n","# > Recall     : 1.000000 | 0.909091\n","# > Specificity: 0.870968 | 0.500000\n","# > Accuracy   : 0.936508 | 0.714286\n","# > AUC        : 0.935484 | 0.704545\n","# > ** Confusion matrix (Train)**\n","# [[27  4]\n","#  [ 0 32]]\n","# > ** Confusion matrix (Test)**\n","# [[ 5  5]\n","#  [ 1 10]]\n","\n","\n","# # 0.20 train_test_split\n","# SCORING_METRICS = 'f1'\n","# PICK_TOP_N_MODELS = 3\n","# CV_COUNT = 3 \n","# POLYNOMIAL_DEGREE = 2\n","\n","# All features: 0.6470588235294118\n","# > Scores     <train>    | <test>\n","# > F1         : 0.825000 | 0.727273\n","# > Precision  : 0.717391 | 0.615385\n","# > Recall     : 0.970588 | 0.888889\n","# > Specificity: 0.606061 | 0.375000\n","# > Accuracy   : 0.791045 | 0.647059\n","# > AUC        : 0.788324 | 0.631944\n","# > ** Confusion matrix (Train)**\n","# [[20 13]\n","#  [ 1 33]]\n","# > ** Confusion matrix (Test)**\n","# [[3 5]\n","#  [1 8]]\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RjeiRFRBHbPc","colab_type":"text"},"source":["###  Not using the hand tuned models \n","\n","The hand tuned models ended up not working very well because they are overfitting and works very well on training data (of certain train test split), but not test data. So we are using the ones found earlier instead. "]},{"cell_type":"code","metadata":{"id":"H6F0E3lDKjKy","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}